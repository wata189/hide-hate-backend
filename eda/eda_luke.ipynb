{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LukeForSequenceClassification were not initialized from the model checkpoint at studio-ousia/luke-japanese-base-lite and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize train\n",
      "tokenize val\n",
      "tokenize tweet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\NeuroDive\\.venv\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\watar\\AppData\\Local\\Temp\\ipykernel_39044\\1306694390.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEST_SIZE = 0.2\n",
    "objectiv_col = \"isFake\"\n",
    "#トレーニングデータと検証用のテストデータに分割\n",
    "def split_data_frame(df:pandas.DataFrame, objectiv_col:str):\n",
    "  df_train, df_val =train_test_split(df, test_size=TEST_SIZE, random_state=42)\n",
    "  train_y = df_train[objectiv_col]\n",
    "  train_x = df_train.drop(objectiv_col, axis=1)\n",
    "\n",
    "  val_y = df_val[objectiv_col]\n",
    "  val_x = df_val.drop(objectiv_col, axis=1)\n",
    "  return [train_x, train_y, val_x, val_y]\n",
    "\n",
    "def clean_text(text):\n",
    "  return text.replace(' ', '').replace('　', '').replace('__BR__', '\\n').replace('\\xa0', '').replace('\\r', '').lstrip('\\n')\n",
    "\n",
    "df_hate_train = pandas.read_csv(\"hate/train.csv\", index_col=0)\n",
    "df_hate_train.drop(\"source\", axis=1)\n",
    "df_hate_test = pandas.read_csv(\"hate/test.csv\", index_col=0)\n",
    "df_hate_test.drop(\"source\", axis=1)\n",
    "df_hate_train['text'] = df_hate_train['text'].apply(clean_text)\n",
    "df_hate_test['text'] = df_hate_test['text'].apply(clean_text)\n",
    "\n",
    "train_x, train_y, val_x, val_y = split_data_frame(df_hate_train, \"label\")\n",
    "\n",
    "df_tweet = pandas.read_csv(\"hate/tweet.csv\", index_col=0)\n",
    "df_tweet['text'] = df_tweet['text'].apply(clean_text)\n",
    "\n",
    "## luke\n",
    "#難点；計算にGPU必要\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "from transformers import (\n",
    "    AutoTokenizer, Trainer, TrainingArguments,\n",
    "    LukeTokenizer, LukeForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_metric\n",
    "MODEL_NAME = \"studio-ousia/luke-japanese-base-lite\"\n",
    "model = LukeForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "tokenizer = LukeTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"tokenize train\")\n",
    "train_vectors = tokenizer( train_x['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "print(\"tokenize val\")\n",
    "val_vectors   = tokenizer(   val_x['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "print(\"tokenize tweet\")\n",
    "tweet_vectors = tokenizer(df_tweet['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"トークン入力データセット\"\"\"\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            input[\"labels\"] = torch.tensor(self.labels[index])\n",
    "\n",
    "        return input\n",
    "\n",
    "#train/valid/testのデータセットをそれぞれ作成する \n",
    "train_ds = MyDataset(train_vectors, train_y.tolist())\n",
    "val_ds   = MyDataset(val_vectors, val_y.tolist())\n",
    "tweet_ds = MyDataset(tweet_vectors)\n",
    "\n",
    "metric_name = \"roc_auc\"\n",
    "metric = load_metric(metric_name, trust_remote_code=True)\n",
    "import numpy\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    predictions, labels = pred\n",
    "    predictions = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "    # 'micro', 'macro', etc. are for multi-label classification. If you are running a binary classification, leave it as default or specify \"binary\" for average\n",
    "    roc_auc = metric.compute(prediction_scores=predictions, references=labels, average=\"binary\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"roc_auc\": roc_auc[\"roc_auc\"]\n",
    "    }\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='luke_log',\n",
    "    overwrite_output_dir        = False, #logを上書きするか\n",
    "    load_best_model_at_end      = True, #EarlyStoppingを使用するならTrue\n",
    "    metric_for_best_model       = metric_name, #EarlyStoppingの判断基準。7-1. compute_metricsのものを指定\n",
    "    save_total_limit            = 1, #output_dirに残すチェックポイントの数\n",
    "    save_strategy               = \"steps\", #いつ保存するか？\n",
    "    evaluation_strategy         = \"steps\", #いつ評価するか？\n",
    "    logging_strategy            = \"steps\", #いつLOGに残すか？\n",
    "    label_names                 = ['labels'], #分類ラベルのkey名称(デフォルトはlabelsなので注意)\n",
    "    lr_scheduler_type           = \"linear\", #学習率の減衰設定(デフォルトlinearなので設定不要)\n",
    "    learning_rate               = 5e-5, #学習率(デフォルトは5e-5)\n",
    "    num_train_epochs            = 2, #epoch数\n",
    "    per_device_train_batch_size = 16, #学習のバッチサイズ\n",
    "    per_device_eval_batch_size  = 12, #バリデーション/テストのバッチサイズ\n",
    "    seed                        = 42, #seed\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, #モデル\n",
    "    args=train_args, #TrainingArguments\n",
    "    tokenizer=tokenizer, #tokenizer\n",
    "    train_dataset=train_ds, #学習データセット\n",
    "    eval_dataset=val_ds, #validデータセット\n",
    "    compute_metrics = compute_metrics, #compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#trainer.predictで評価可能\n",
    "print(\"predict\")\n",
    "print(\"正解率:\" + str(accuracy_score(df_tweet[\"label\"], trainer.predict(tweet_ds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
