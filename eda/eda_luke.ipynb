{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LukeForSequenceClassification were not initialized from the model checkpoint at studio-ousia/luke-japanese-base-lite and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize train\n",
      "tokenize val\n",
      "tokenize tweet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\workspace\\NeuroDive\\.venv\\lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\watar\\AppData\\Local\\Temp\\ipykernel_39044\\1306694390.py:66: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEST_SIZE = 0.2\n",
    "objectiv_col = \"isFake\"\n",
    "#ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ç”¨ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²\n",
    "def split_data_frame(df:pandas.DataFrame, objectiv_col:str):\n",
    "  df_train, df_val =train_test_split(df, test_size=TEST_SIZE, random_state=42)\n",
    "  train_y = df_train[objectiv_col]\n",
    "  train_x = df_train.drop(objectiv_col, axis=1)\n",
    "\n",
    "  val_y = df_val[objectiv_col]\n",
    "  val_x = df_val.drop(objectiv_col, axis=1)\n",
    "  return [train_x, train_y, val_x, val_y]\n",
    "\n",
    "def clean_text(text):\n",
    "  return text.replace(' ', '').replace('ã€€', '').replace('__BR__', '\\n').replace('\\xa0', '').replace('\\r', '').lstrip('\\n')\n",
    "\n",
    "df_hate_train = pandas.read_csv(\"hate/train.csv\", index_col=0)\n",
    "df_hate_train.drop(\"source\", axis=1)\n",
    "df_hate_test = pandas.read_csv(\"hate/test.csv\", index_col=0)\n",
    "df_hate_test.drop(\"source\", axis=1)\n",
    "df_hate_train['text'] = df_hate_train['text'].apply(clean_text)\n",
    "df_hate_test['text'] = df_hate_test['text'].apply(clean_text)\n",
    "\n",
    "train_x, train_y, val_x, val_y = split_data_frame(df_hate_train, \"label\")\n",
    "\n",
    "df_tweet = pandas.read_csv(\"hate/tweet.csv\", index_col=0)\n",
    "df_tweet['text'] = df_tweet['text'].apply(clean_text)\n",
    "\n",
    "## luke\n",
    "#é›£ç‚¹ï¼›è¨ˆç®—ã«GPUå¿…è¦\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchinfo import summary\n",
    "from transformers import (\n",
    "    AutoTokenizer, Trainer, TrainingArguments,\n",
    "    LukeTokenizer, LukeForSequenceClassification,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_metric\n",
    "MODEL_NAME = \"studio-ousia/luke-japanese-base-lite\"\n",
    "model = LukeForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "tokenizer = LukeTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"tokenize train\")\n",
    "train_vectors = tokenizer( train_x['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "print(\"tokenize val\")\n",
    "val_vectors   = tokenizer(   val_x['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "print(\"tokenize tweet\")\n",
    "tweet_vectors = tokenizer(df_tweet['text'].tolist(), return_tensors='pt', padding=\"max_length\", truncation=True)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"ãƒˆãƒ¼ã‚¯ãƒ³å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\"\"\"\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = {key: torch.tensor(val[index]) for key, val in self.encodings.items()}\n",
    "        if self.labels is not None:\n",
    "            input[\"labels\"] = torch.tensor(self.labels[index])\n",
    "\n",
    "        return input\n",
    "\n",
    "#train/valid/testã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãã‚Œãã‚Œä½œæˆã™ã‚‹ \n",
    "train_ds = MyDataset(train_vectors, train_y.tolist())\n",
    "val_ds   = MyDataset(val_vectors, val_y.tolist())\n",
    "tweet_ds = MyDataset(tweet_vectors)\n",
    "\n",
    "metric_name = \"roc_auc\"\n",
    "metric = load_metric(metric_name, trust_remote_code=True)\n",
    "import numpy\n",
    "\n",
    "def compute_metrics(pred):\n",
    "\n",
    "    predictions, labels = pred\n",
    "    predictions = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "    # 'micro', 'macro', etc. are for multi-label classification. If you are running a binary classification, leave it as default or specify \"binary\" for average\n",
    "    roc_auc = metric.compute(prediction_scores=predictions, references=labels, average=\"binary\")\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"roc_auc\": roc_auc[\"roc_auc\"]\n",
    "    }\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='luke_log',\n",
    "    overwrite_output_dir        = False, #logã‚’ä¸Šæ›¸ãã™ã‚‹ã‹\n",
    "    load_best_model_at_end      = True, #EarlyStoppingã‚’ä½¿ç”¨ã™ã‚‹ãªã‚‰True\n",
    "    metric_for_best_model       = metric_name, #EarlyStoppingã®åˆ¤æ–­åŸºæº–ã€‚7-1. compute_metricsã®ã‚‚ã®ã‚’æŒ‡å®š\n",
    "    save_total_limit            = 1, #output_dirã«æ®‹ã™ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®æ•°\n",
    "    save_strategy               = \"steps\", #ã„ã¤ä¿å­˜ã™ã‚‹ã‹ï¼Ÿ\n",
    "    evaluation_strategy         = \"steps\", #ã„ã¤è©•ä¾¡ã™ã‚‹ã‹ï¼Ÿ\n",
    "    logging_strategy            = \"steps\", #ã„ã¤LOGã«æ®‹ã™ã‹ï¼Ÿ\n",
    "    label_names                 = ['labels'], #åˆ†é¡ãƒ©ãƒ™ãƒ«ã®keyåç§°(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯labelsãªã®ã§æ³¨æ„)\n",
    "    lr_scheduler_type           = \"linear\", #å­¦ç¿’ç‡ã®æ¸›è¡°è¨­å®š(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆlinearãªã®ã§è¨­å®šä¸è¦)\n",
    "    learning_rate               = 5e-5, #å­¦ç¿’ç‡(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯5e-5)\n",
    "    num_train_epochs            = 2, #epochæ•°\n",
    "    per_device_train_batch_size = 16, #å­¦ç¿’ã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    per_device_eval_batch_size  = 12, #ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³/ãƒ†ã‚¹ãƒˆã®ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    seed                        = 42, #seed\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, #ãƒ¢ãƒ‡ãƒ«\n",
    "    args=train_args, #TrainingArguments\n",
    "    tokenizer=tokenizer, #tokenizer\n",
    "    train_dataset=train_ds, #å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    eval_dataset=val_ds, #validãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "    compute_metrics = compute_metrics, #compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#trainer.predictã§è©•ä¾¡å¯èƒ½\n",
    "print(\"predict\")\n",
    "print(\"æ­£è§£ç‡:\" + str(accuracy_score(df_tweet[\"label\"], trainer.predict(tweet_ds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
