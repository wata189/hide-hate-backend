{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEST_SIZE = 0.2\n",
    "FOLDER_PATH = \"../data\"\n",
    "#トレーニングデータと検証用のテストデータに分割\n",
    "def split_data_frame(df:pandas.DataFrame, objectiv_col:str):\n",
    "    df_train, df_val =train_test_split(df, test_size=TEST_SIZE, random_state=42)\n",
    "    train_y = df_train[objectiv_col]\n",
    "    train_x = df_train.drop(objectiv_col, axis=1)\n",
    "\n",
    "    val_y = df_val[objectiv_col]\n",
    "    val_x = df_val.drop(objectiv_col, axis=1)\n",
    "    return [train_x, train_y, val_x, val_y]\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(' ', '').replace('　', '').replace('__BR__', '\\n').replace('\\xa0', '').replace('\\r', '').lstrip('\\n')\n",
    "\n",
    "df_hate_train = pandas.read_csv(FOLDER_PATH + \"/train.csv\", index_col=0)\n",
    "df_hate_train.drop(\"source\", axis=1)\n",
    "df_hate_test = pandas.read_csv(FOLDER_PATH + \"/test.csv\", index_col=0)\n",
    "df_hate_test.drop(\"source\", axis=1)\n",
    "df_hate_train['text'] = df_hate_train['text'].apply(clean_text)\n",
    "df_hate_test['text'] = df_hate_test['text'].apply(clean_text)\n",
    "df_tweet = pandas.read_csv(FOLDER_PATH + \"/tweet.csv\", index_col=0)\n",
    "df_tweet['text'] = df_tweet['text'].apply(clean_text)\n",
    "\n",
    "train_x, train_y, val_x, val_y = split_data_frame(df_hate_train, \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForPreTraining were not initialized from the model checkpoint at ALINEAR/albert-japanese-v2 and are newly initialized: ['sop_classifier.classifier.bias', 'sop_classifier.classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer, AlbertForPreTraining\n",
    "import torch\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('ALINEAR/albert-japanese-v2')\n",
    "model = AlbertForPreTraining.from_pretrained('ALINEAR/albert-japanese-v2')\n",
    "\n",
    "count = 0\n",
    "max = len(train_x[\"text\"]) + len(val_x[\"text\"]) + len(df_tweet[\"text\"])\n",
    "print_num = max % 10\n",
    "\n",
    "def vectorize(text:str):\n",
    "  # 進捗表示\n",
    "  global count\n",
    "  count += 1\n",
    "  if count % 10 == print_num:\n",
    "    print(\"\\r\", f\"{count}/{max}\", end=\"\")\n",
    "\n",
    "  input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "  outputs = model(input_ids)\n",
    "  return torch.flatten(outputs.prediction_logits).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 567/5307"
     ]
    }
   ],
   "source": [
    "count = 0 # 進捗のスタートリセットするため\n",
    "train_vectors = train_x[\"text\"].apply(vectorize)\n",
    "val_vectors = val_x[\"text\"].apply(vectorize)\n",
    "tweet_vectors = df_tweet[\"text\"].apply(vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def print_probas(y, probas):\n",
    "  fpr, tpr, threshold = roc_curve(y, probas[:, 1])\n",
    "  plt.style.use(\"fivethirtyeight\")\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.set_size_inches(4.8, 5)\n",
    "\n",
    "  ax.step(fpr, tpr, \"gray\")\n",
    "  ax.fill_between(fpr, tpr, 0, color=\"skyblue\", alpha=0.8)\n",
    "  ax.set_xlabel(\"False Positive Rate\")\n",
    "  ax.set_ylabel(\"True Positive Rate\")\n",
    "  ax.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "  print(\"AUC:\" + str(roc_auc_score(y, probas[:, 1])))\n",
    "  plt.show()\n",
    "\n",
    "def calculate(model):\n",
    "  # fit行う\n",
    "  model.fit(train_vectors, train_y)\n",
    "  predict(model)\n",
    "\n",
    "def predict(model):\n",
    "  print(\"-----------------------train.csvデータ----------------\")\n",
    "  print(\"正解率:\" + str(accuracy_score(val_y, model.predict(val_vectors))))\n",
    "  print_probas(val_y, model.predict_proba(val_vectors))\n",
    "  print(\"-----------------------tweet.csvデータ----------------\")\n",
    "  print(\"正解率:\" + str(accuracy_score(df_tweet[\"label\"], model.predict(tweet_vectors))))\n",
    "  print_probas(df_tweet[\"label\"], model.predict_proba(tweet_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニューラルネットワーク\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "calculate(MLPClassifier(hidden_layer_sizes=(16,), random_state=42))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
