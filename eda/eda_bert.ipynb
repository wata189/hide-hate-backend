{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEST_SIZE = 0.2\n",
    "#トレーニングデータと検証用のテストデータに分割\n",
    "def split_data_frame(df:pandas.DataFrame, objectiv_col:str):\n",
    "    df_train, df_val =train_test_split(df, test_size=TEST_SIZE, random_state=42)\n",
    "    train_y = df_train[objectiv_col]\n",
    "    train_x = df_train.drop(objectiv_col, axis=1)\n",
    "\n",
    "    val_y = df_val[objectiv_col]\n",
    "    val_x = df_val.drop(objectiv_col, axis=1)\n",
    "    return [train_x, train_y, val_x, val_y]\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(' ', '').replace('　', '').replace('__BR__', '\\n').replace('\\xa0', '').replace('\\r', '').lstrip('\\n')\n",
    "\n",
    "df_hate_train = pandas.read_csv(\"hate/train.csv\", index_col=0)\n",
    "df_hate_train.drop(\"source\", axis=1)\n",
    "df_hate_test = pandas.read_csv(\"hate/test.csv\", index_col=0)\n",
    "df_hate_test.drop(\"source\", axis=1)\n",
    "df_hate_train['text'] = df_hate_train['text'].apply(clean_text)\n",
    "df_hate_test['text'] = df_hate_test['text'].apply(clean_text)\n",
    "df_tweet = pandas.read_csv(\"hate/tweet.csv\", index_col=0)\n",
    "df_tweet['text'] = df_tweet['text'].apply(clean_text)\n",
    "\n",
    "train_x, train_y, val_x, val_y = split_data_frame(df_hate_train, \"label\")\n",
    "\n",
    "# bert\n",
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "class SentenceBertJapanese:\n",
    "    def __init__(self, model_name_or_path, device=None):\n",
    "        self.tokenizer = BertJapaneseTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = BertModel.from_pretrained(model_name_or_path)\n",
    "        self.model.eval()\n",
    "\n",
    "        if device is None:\n",
    "            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def _mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "    def encode(self, sentences, batch_size=8):\n",
    "        all_embeddings = []\n",
    "        iterator = range(0, len(sentences), batch_size)\n",
    "        for batch_idx in iterator:\n",
    "            batch = sentences[batch_idx:batch_idx + batch_size]\n",
    "\n",
    "            encoded_input = self.tokenizer.batch_encode_plus(batch, padding=\"longest\", truncation=True, return_tensors=\"pt\").to(self.device)\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = self._mean_pooling(model_output, encoded_input[\"attention_mask\"]).to('cpu')\n",
    "\n",
    "            all_embeddings.extend(sentence_embeddings)\n",
    "\n",
    "        # return torch.stack(all_embeddings).numpy()\n",
    "        return torch.stack(all_embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"sonoisa/sentence-bert-base-ja-mean-tokens-v2\"\n",
    "bert = SentenceBertJapanese(MODEL_NAME)\n",
    "\n",
    "print(\"bert model create\")\n",
    "\n",
    "train_vectors = bert.encode(train_x['text'])\n",
    "val_vectors = bert.encode(val_x['text'])\n",
    "tweet_vectors = bert.encode(df_tweet['text'])\n",
    "\n",
    "\n",
    "print(\"text vectors create\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def print_probas(y, probas):\n",
    "  fpr, tpr, threshold = roc_curve(y, probas[:, 1])\n",
    "  plt.style.use(\"fivethirtyeight\")\n",
    "  fig, ax = plt.subplots()\n",
    "  fig.set_size_inches(4.8, 5)\n",
    "\n",
    "  ax.step(fpr, tpr, \"gray\")\n",
    "  ax.fill_between(fpr, tpr, 0, color=\"skyblue\", alpha=0.8)\n",
    "  ax.set_xlabel(\"False Positive Rate\")\n",
    "  ax.set_ylabel(\"True Positive Rate\")\n",
    "  ax.set_facecolor(\"xkcd:white\")\n",
    "\n",
    "  print(\"AUC:\" + str(roc_auc_score(y, probas[:, 1])))\n",
    "  plt.show()\n",
    "\n",
    "def calculate(model):\n",
    "  # fit行う\n",
    "  model.fit(train_vectors, train_y)\n",
    "  predict(model)\n",
    "\n",
    "def predict(model):\n",
    "  print(\"-----------------------train.csvデータ----------------\")\n",
    "  print(\"正解率:\" + str(accuracy_score(val_y, model.predict(val_vectors))))\n",
    "  print_probas(val_y, model.predict_proba(val_vectors))\n",
    "  print(\"-----------------------tweet.csvデータ----------------\")\n",
    "  print(\"正解率:\" + str(accuracy_score(df_tweet[\"label\"], model.predict(tweet_vectors))))\n",
    "  print_probas(df_tweet[\"label\"], model.predict_proba(tweet_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ニューラルネットワーク\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "calculate(MLPClassifier(hidden_layer_sizes=(16,), random_state=42))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
